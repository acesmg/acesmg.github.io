<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Boosting - 
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:21stacks.top ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        <li id="menu_item_index"><a href="index.html">HOME</a></li>
        <li id="menu_item_archives"><a href="archives.html">Archives</a></li>
        <li id="menu_item_about"><a href="about.html">ABOUT</a></li>
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; </span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="%E8%81%9A%E5%90%88%E6%96%B9%E6%B3%95.html">组合方法</a></li>
        
            <li><a href="%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.html">线性模型</a></li>
        
            <li><a href="%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html">Optimization</a></li>
        
            <li><a href="%E9%9B%B6%E7%A2%8E.html">零碎</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Boosting</h1>
     
        <div class="read-more clearfix">
          <span class="date">2016/3/29</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='%E8%81%9A%E5%90%88%E6%96%B9%E6%B3%95.html'>组合方法</a></span>
           
         
          <span class="comments">
            

            
              &nbsp;<a  class="ds-thread-count" data-thread-key="14592400407522.html" data-count-type="comments" href="14592400407522.html#ds-thread"></a>
            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <p>　　Boosting是一种Adaptive Basis-function Model (ABM)。ABM模型可以用式(1)高度概括：<br/>
\[f(x)=w_0+\sum_{m=1}^{M} w_m \phi_m(x) \qquad(1)\]<br/>
　　这里的\(\phi_m(x)\)是从数据中学习而来的第\(m\)个基本模型(基函数)。ABM通过多个模型的线性组合来得到<strong>非线性</strong>的模型。事实上，决策树就是一种ABM。一棵具有\(M\)个叶子节点的决策树可以表示为：<br/>
\[T(x)=\sum_{m=1}^{M}w_mI(x\in R_m)\qquad (2)\]<br/>
　　\(R_m\)是决策树的第\(m\)个叶节点；\(w_m\)表示第\(m\)个叶节点的标示值(输出)；当样本x属于第\(m\)个叶节点时，函数\(I(x \in R_m)\)输出1，否则输出0。<br/>
　　Boosting是除了决策树以外的另一种常见的ABM模型。Boosting中的基本模型也叫做weak learner。被使用最多的基本模型是CART。</p>

<span id="more"></span><!-- more -->

<h2 id="toc_0">前向分步算法</h2>

<p>　　为了得到一个ABM模型，需要求数个基函数，以及它们对应的系数。Boosting方法的核心是前向分步算法。前向分步的意思就是，逐步往前构建，且不回头来调整。若最终要达成的模型含有\(M\)个基函数，则前向分步算法要经过\(M\)次迭代。在第\(m\)次(\(0&lt;m\leq M\))迭代时通过以下公式求得\(f_m\)：<br/>
\[f_{m}(x)=f_{m-1}(x)+\alpha_m\phi_m(x) \qquad(3)\]<br/>
而最终的模型就是\(f(x)=f_M(x)\)。这里用\(\alpha_m\)代替了式(1)中的\(w_m\)。<br/>
　　在求\(f_m\)时，我们已经得到了\(f_{m-1}\)。因此只要得到第\(m\)个基函数\(\phi_m(x)\)及其系数\(\alpha_m\)，就可以通过(3)式构造出\(f_m\)。Boosting在第\(m\)次迭代中通过极小化损失函数来求\(f_m(x)\)。其优化目标可用下式表示：<br/>
\[\min_{f_m}=\sum_{i=1}^{N}L(y_i,f_m(x_i))\qquad (4)\]<br/>
　　上式中，\(L\)是损失函数，\(N\)是训练集的大小。具体的Boosting算法有很多种，它们采用了各种不同的损失函数。<br/>
　　把(3)带入(4)式，我们的优化目标可以写成：<br/>
\[(\phi_m,\alpha_m)=\min_{\phi,\alpha}\sum_{i=1}^{N}L(y_i,f_{m-1}(x)+\alpha\phi(x)) \qquad(5)\]<br/>
　　通过以上表述可以知道，Boosting是一种贪心策略，在迭代的过程中逐步逼近优化目标，求得模型的不是Global Optimum。</p>

<h2 id="toc_1">AdaBoost</h2>

<p>　　AdaBoost(Adaptive Boost)是最常见的一种Boosting算法。它被认为是最好的out-of-the-box(咋翻译？)分类算法。AdaBoost为每个训练样本赋予了不同的权值，表示它们的重要性(分类的难易程度)。在训练过程中，随着迭代的进行，难以分类的样本权值会变得更高，使模型逐步倾向这些样本。</p>

<h4 id="toc_2">推理</h4>

<p>　　一下讨论基于二分类问题，令\(y_i\in\{-1,1\}\)。<br/>
　　AdaBoost采用的损失函数是指数损失函数。在前向分步算法的第\(m\)步，优化的目标函数就是式(6)：<br/>
\[L_m(\phi)=\sum_{i=1}^{N} exp[-y_i(f_{m-1}(x_i)+\alpha\phi(x_i))]\qquad(6)\]<br/>
　　下面要描述的是如何通过优化(6)来求\(\alpha\)和\(\phi(x)\)。<br/>
　　首先，令<br/>
\[w_{m,i}=exp[-y_if_{m-1}(x_i)]\qquad(7)\]<br/>
\(w_{m,i}\)表示开始第\(m\)次迭代时，第\(i\)个训练样本的权重。于是(6)可以表示为：<br/>
\[L_m(\phi)=\sum_{i=1}^{N}w_{m,i}exp(-\alpha y_i \phi(x_i))\qquad(8)\]<br/>
此时如果把训练样本输入到基本模型\(\phi(x)\)，则有的样本会被正确分类，另一些则被错误分类。在上式中把这两类样本分开来，可以改写成：<br/>
\[L_m=e^{-\alpha}\sum_{y_i=\phi(x_i)}^{}w_{m,i}+e^{\alpha}\sum_{y_i\neq \phi(x_i)}^{}w_{m,i}\\\<br/>
=(e^{\alpha}-e^{-\alpha})\sum_{i=1}^{N}w_{m,i}I(y_i\neq \phi(x_i))+e^{-\alpha}\sum_{i=1}^{N}w_{m,i}<br/>
\qquad(9)\]<br/>
因为\(\alpha\geq 0\)，所以\((e^{\alpha}-e^{-\alpha})\geq 0\)。于是可得，能使\(L_m\)极小化的\(\phi_m\)就如<strong>(10)</strong>所示：<br/>
\[\phi_m(x)=arg\min_{\phi}\sum_{i=1}^{N}w_{m,i}I(y_i \neq \phi(x_i))\qquad(10)\]<br/>
上式中，\(\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x))\)可以定义为\(\phi_M(x)\)在训练集上的加权分类误差\(e_m\)。<br/>
\[e_m=\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x))\qquad(11)\]<br/>
　　接着来求\(\phi_m(x)\)的系数\(\alpha_m\)。在式(9)上对\(\alpha\)求导，令导数为0：<br/>
\[\frac{d L_m}{d \alpha}=e^{\alpha}\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x))-e^{-\alpha}\sum_{i=1}^{N}w_{m,i}I(y_i=\phi_m(x_i))=0\qquad(12)\]<br/>
化简可得：<br/>
\[\alpha=\frac{1}{2}ln{\frac{\sum_{i=1}^{N}w_{m,i}I(y_i=\phi_m(x_i))}{\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x_i))}}\qquad(13)\]<br/>
式(13)中，分子是所有被\(\phi_m(x)\)正确分类的样本的权值之和，分母是所有被\(\phi_m(x)\)误分类的样本的权值之和，因此有：<br/>
\[\alpha_m=\frac{1}{2}ln{\frac{1-e_m}{e_m}}\qquad(14)\]<br/>
　　权值\(\alpha_m\)表示模型\(\phi_m(x)\)在最终的组合模型中的话语权大小。当\(e_m&lt;0.5\)时，\(\alpha_m&gt;0\)，且\(\alpha_m\)随着\(e_m\)的减小而增大，即误差越小的模型越权威。<br/>
　　到此我们找到了极小化\(L_m\)的weak learner \(\phi_m(x)\)和它的权重系数\(\alpha_m\)。顺势可以使用式(3)求得在第\(m\)次迭代时的\(f_m(x)\)。<br/>
　　前面我们令\(w_{m,i}=exp[-y_i f_{m-1}(x_i)]\)，即每一次都使用上一次迭代得到的\(f_{m-1}(x)\)来更新训练集样本的权值。若已求得\(f_{m}(x)\)，则在第\({m+1}\)次迭代求\(w_{m+1,i}\)的方法如下：<br/>
\[w_{m+1,i}=exp[-y_i f_{m}(x_i)]\\<br/>
=exp[-y_i(f_{m-1}(x_i) +\alpha_m\phi_m(x_i)  )]\\<br/>
=w_{m,i} * e^{-y_i \alpha_m \phi_m(x_i)}\qquad\quad(15)\]<br/>
　　上式的效果就是让那些在\(\phi_m(x)\)上被正确分类的样本对应的权值变小，使错误分类的样本的权值变大。这就使得后续的学习偏重于处理较难分类的样本。<br/>
　　为了让\(w_{m,i}\)成为一种概率分布，还要进一步正规化：<br/>
\[w_{m+1,i}=\frac{w_{m,i}exp[-y_i\alpha_m\phi_m(x_i)]}{Z_m}\qquad(16)\\<br/>
Z_m=\sum_{i=1}^{N}w_{m,i}exp[-y_i\alpha_m\phi_m(x_i)]\qquad(17)<br/>
\]<br/>
　　在第一次迭代时要用到\(f_0(x)\)，因此预先定义\(f_0(x)=0\)(一个常数)。于是有\(w_{1,i}=exp[-y_if_0(x_i)]=1\)，对\(i=1,2,...N\)。正规化后有\(w_{1,i}=\frac{1}{N}\)。下一部分总结性地描述了AdaBoost算法。<br/>
　　</p>

<h4 id="toc_3">算法 Adaboost</h4>

<p><strong>输入</strong>：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)，\(y_i\in\{-1,+1\}\)。以及弱基本模型\(\phi(x)\)；迭代次数\(M\)。<br/>
<strong>输出</strong>：强分类器\(\Phi(x)\)。</p>

<p>1.初始化训练样本的权值分布：<br/>
\[D_1=(w_{11},...,w_{1i},...w_{1N}),\quad w_{1i}=\frac{1}{N},\quad i=1,2,...N\]<br/>
2.对\(m=1,2,...M\)<br/>
　　a.使用训练数据集和它的权值分布\(D_m\)训练学习模型，得到弱基本模型\(\phi_m(x)\)，此模型满足式(10)。<br/>
　　b.根据式(11)计算\(\phi_m(x)\)在训练数据集上的带权误差<br/>
　　\[e_m=\sum_{i=1}^N w_{m,i}I(\phi_m(x_i)\neq y_i)\]<br/>
　　c.根据式(14)计算\(\phi_m(x)\)的权重系数<br/>
　　\[\alpha_m=\frac{1}{2}ln\frac{1-e_m}{e_m}\]<br/>
　　d.根据式(16)和(17)更新训练数据集的权值分布<br/>
\[D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...w_{m+1,N})\\\<br/>
w_{m+1,i}=\frac{w_{m,i} exp(-y_i \alpha_m \phi_m(x_i))}{Z_m}\\\<br/>
Z_m=\sum_{i=1}^{N} w_{m,i}exp(-y_i \alpha_m h_m(x_i))\]<br/>
3.组合得到最终分类器<br/>
\[\Phi(x)=sign(f_M(x))=sign(\sum_{m=1}^{M}\alpha_m \phi_m(x))\]</p>

<h4 id="toc_4">局限性</h4>

<p>　　AdaBoost容易偏向于难以分类的样本，因此对Outlier很敏感，容易过拟合。</p>

<h2 id="toc_5">其他Boosting算法</h2>

<h4 id="toc_6">L2Boosting</h4>

<p>　　在Boosting的前向分步算法中，若采用平方误差损失函数，就得到了L2Boosting模型。在L2Boosting模型中，weak learner的权重系数通常都设为1。因此在前向分步算法的第\(m\)次迭代时要优化的目标就是求：<br/>
\[\phi_m=arg\min_{\phi}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\phi(x_i))\] <br/>
这里\(L(y,f_{m-1}(x)+\phi(x))=[y-f_{m-1}(x)-\phi(x)]^2\)。也就是说，要寻求一个\(\phi_m\)，使得\(\phi_m(x)\)尽可能地靠近(拟合)\(y-f_{m-1}(x)\)。令\(r=y-f_{m-1}(x)\)为当前模型(\(f_{m-1}(x)\))拟合训练样本的<strong>残差</strong>。在第\(m\)次迭代时，样本\(i\)对应的残差为：<br/>
\[r_{m,i}=y_i-f_{m-1}(x_i)\]<br/>
　　L2Boosting算法的迭代过程，就是通过拟合残差来学习新的weak learner，并把这些weak learner线性组合以得到强模型。</p>

<h2 id="toc_7">梯度提升Gradient Boosting</h2>

<p>　　Boosting算法可以采用各种不同的损失函数。这些算法可以归结为一种统一的形式，即梯度提升算法。梯度提升算法可以使用任意的合理的<strong>可微</strong>损失函数。对Boosting模型，我们在每一次迭代时都是求\(\phi_m(x)\)和\(\alpha_m\)，尽可能地使<br/>
　　\[f_{m+1}(x)=f_m(x)+\alpha_m\phi_m(x)\rightarrow y\]<br/>
即尽可能地让\(\alpha_m(x_i)\phi_m(x_i)\rightarrow y_i-f_m(x_i)\)。<br/>
上式右边正是残差。如果采用损失函数：<br/>
\[L=\frac{1}{2}\sum_{i=1}^{N}(y_i-f_m(x_i))^2\]<br/>
则此残差就是损失函数的负梯度值：<br/>
\[-\frac{\partial L}{\partial f_m(x_i)}=y-f_m(x_i)\]<br/>
　　对于一般化的损失函数，梯度提升算法采用损失函数负梯度值作为残差的近似值(<strong>伪残差</strong>)。在训练样本(输入向量)及其伪残差形成的数据集上训练新的weak learner。<br/>
　　像其他的Boosting算法一样，梯度提升算法也采用前向分步算法。在第\(m\)次迭代时，要求weak learner \(\phi_m(x)\)以及它对应的权重系数\(\alpha_m\)，以极小化损失函数为目标。梯度提升通过拟合伪残差来得到\(\phi_m(x)\)，然后求解一维的(以\(\alpha_m\)为自变量)优化问题得到\(\alpha_m\)。<br/>
　　</p>

<h4 id="toc_8">算法 Gradient Boosting</h4>

<p><strong>输入</strong>：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)。以及weak learner模型\(\phi(x)\)；迭代次数\(M\)。<br/>
<strong>输出</strong>：强模型\(f_M(x)\)。</p>

<p>1.初始化模型\(f_0(x)\)(一个常数)：<br/>
\[f_0(x)=arg\min_{\alpha}\sum_{i=1}^{N}L(y_i,\alpha)\]<br/>
2.对\(m=1,2,...M\)：<br/>
　　a.求各训练样本的伪残差：<br/>
\[r_{m,i}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\]<br/>
　　b.在训练集\(\{(x_1,r_{m,1}),(x_2,r_{m,2}),...(x_N,r_{m,N})\}\)上训练基本模型，得到\(\phi_m(x)\)。<br/>
　　c.解一维优化问题，求\(\alpha_m\)<br/>
\[\alpha_m=arg\min_{\alpha}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\alpha\phi_m(x_i))\]<br/>
　　d.更新模型<br/>
\[f_m(x)=f_{m-1}(x)+\alpha_m\phi_m(x)\]<br/>
3.输出 \(f_M(x)\)。</p>

<h4 id="toc_9">正则化</h4>

<p>　　为了提高模型的泛化能力，可以采用多种正则化策略。<br/>
　　1.限制迭代次数\(M\)。迭代次数决定了基本模型的数量。较大的\(M\)能减小训练误差，但容易过拟合。可以单独采用一个验证集，在迭代过程中评估泛化误差。<br/>
　　2.Shrinkage。给新学得的模型加一个较小的系数，以限制其对最终模型的影响：<br/>
\[f_m(x)=f_{m-1}(x)+ v\alpha_m\phi_m(x)\qquad 0&lt;v\leq 1\]<br/>
这个\(v\)也就是learning rate。较小的\(v\)会减慢算法的收敛速度，但能得到更好的模型。<br/>
　　2.随机梯度提升。在迭代过程中每次都从原始训练集选择一个子集。通常抽取原数据集一半大小的子集。<br/>
　　3.当采用决策树作为基本模型时，可以限制叶节点内的样本数。若节点样本数小于一定值，则不再划分。</p>

<h2 id="toc_10">GBDT</h2>

<p>　　Gradient Boosting如果采用决策树作为基本模型，就得到了Gradient Boosting Decision Tree(GBDT)。也就是说在梯度提升算法的迭代过程中，每次要找的基本模型就是一棵树。树的叶节点们把输入空间划分成多个子区域，并对各子区域分别估计响应值(如对回归问题求落入该区域样本y值的平均，对分类问题采取多数投票)。设\(\phi_m(x)\)是一棵决策树，它把输入空间划分成\(J\)个区域，则\(\phi_m(x)\)可以表示为<br/>
\[\phi_m(x)=\sum_{j=1}^{J}c_{m,j}I(x\in R_{m,j})\]<br/>
这里\(R_{m,j}\)表示树\(\phi_m(x)\)的第\(j\)个叶节点(输入空间子区域)，\(c_{m,j}\)是其对应的响应值。<br/>
　　这也就是说，GBDT对梯度提升算法做了改进，不再是给模型\(\phi_m(x)\)一个参数\(\alpha_m\)，而是给它的所有叶节点分别赋予一个参数。<br/>
　　当设置\(J=2\)，就是采用决策桩(Decision Stump)作为基本模型。通常设置\(4\leq J\leq 8\)。</p>

<h4 id="toc_11">算法 GBDT</h4>

<p><strong>输入</strong>：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)。以及weak learner 决策树模型\(\phi(x)\)；迭代次数\(M\)。<br/>
<strong>输出</strong>：\(f_M(x)\)。</p>

<p>1.初始化模型\(f_0(x)\)：<br/>
\[f_0(x)=arg\min_{\alpha}\sum_{i=1}^{N}L(y_i,\alpha)\]<br/>
2.对\(m=1,2,...M\)：<br/>
　　a.求各训练样本的伪残差：<br/>
\[r_{m,i}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\]<br/>
　　b.在训练集\(\{(x_1,r_{m,1}),(x_2,r_{m,2}),...(x_N,r_{m,N})\}\)上训练，得到决策树模型\(\phi_m(x)\)，它有\(J\)个子区域。<br/>
　　c.对\(j=1,2,...J\)，计算<br/>
　　\[c_{m,j}=arg\min_{c}\sum_{x_i\in R_{m,j}}L(y_i, (f_{m-1}+c))\]<br/>
　　d.更新模型<br/>
\[f_m(x)=f_{m-1}(x)+\sum_{j=1}^{J}c_{m,j}I(x\in R_{m,j})\]<br/>
3.输出模型<br/>
\[f_M(x)=\sum_{m=1}^{M}\phi_m(x)\\\<br/>
=\sum_{m=1}^{M}\sum_{j=1}^{J}c_{m,j}I(x\in R_{m,j})\]</p>

<h2 id="toc_12">Reference</h2>

<blockquote>
<p>[1] 李航，统计学习方法<br/>
[2] Murphy, Machine Learning: A Probabilistic Perspective</p>
</blockquote>

<p>　　</p>

<p>　</p>

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="14598347945676.html" 
          title="Previous Post: 梯度下降">&laquo; 梯度下降</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="14592424184360.html" 
          title="Next Post: Bagging">Bagging &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          
<!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<a class="jiathis_button_tsina"></a>
<a class="jiathis_button_weixin"></a>
<a class="jiathis_button_fb"></a>
<a class="jiathis_button_twitter"></a>
<a class="jiathis_button_googleplus"></a>
<a class="jiathis_button_copy"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
	summary:"",
	shortUrl:false,
	hideMore:true
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

          

          
              <div class="ds-thread" data-thread-key="14592400407522.html" data-url="http://21stacks.top/14592400407522.html" data-title="Boosting"></div>
          
        </div>
      </div>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="http://7xrz9i.com1.z0.glb.clouddn.com/avatar1.jpg" /></div>
            
                <h1></h1>
                <div class="site-des">搬运者Ace</div>
                <div class="social">







<a class="weibo" target="_blank" href="http://weibo.com/acetseng" title="weibo">Weibo</a>

<a class="github" target="_blank" href="https://github.com/AceTseng" title="GitHub">GitHub</a>

  <a class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>
              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="%E8%81%9A%E5%90%88%E6%96%B9%E6%B3%95.html"><strong>组合方法</strong></a>
        
            <a href="%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.html"><strong>线性模型</strong></a>
        
            <a href="%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95.html"><strong>Optimization</strong></a>
        
            <a href="%E9%9B%B6%E7%A2%8E.html"><strong>零碎</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="14605499748686.html">粒子群优化算法</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14600116978074.html">SSE的背后</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14598347945676.html">梯度下降</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14592400407522.html">Boosting</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="14592424184360.html">Bagging</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2016 AceTseng
  </div>
</div>

        </section>
      </div>
    </div>

  


<script type="text/javascript">
var duoshuoQuery = {short_name:'acesmg'};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"><div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"><div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
