<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[]]></title>
  <link href="http://zxcoder.xyz/atom.xml" rel="self"/>
  <link href="http://zxcoder.xyz/"/>
  <updated>2016-04-03T14:21:47+08:00</updated>
  <id>http://zxcoder.xyz/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Bagging]]></title>
    <link href="http://zxcoder.xyz/14592424184360.html"/>
    <updated>2016-03-29T17:06:58+08:00</updated>
    <id>http://zxcoder.xyz/14592424184360.html</id>
    <content type="html"><![CDATA[
<p>　　一种聚合学习模型。<br/>
　　使用训练数据集的不同子集来训练多个模型，最终输出模型是这些模型的平均或者Major Vote。<br/>
　　抽取子集时是有放回的(With replacement)。<br/>
　　设样本集大小为\(N\)，则单个样本被抽中(至少被抽中一次)的概率是：<br/>
　　\[p=1-(1-\frac{1}{N})^{N}\]<br/>
　　当\(N\)足够大时，\(p\approx 0.632\)。因此每抽取一个训练子集，原训练集中大约\(63\%\)的样本会被抽中。<br/>
　　</p>

<h2 id="toc_0">缺陷</h2>

<p>　　组合模型的相关性越低，组合后的效果越好。Bagging策略选择的子集之间的相关性(Correlation)通常较高，因此性能不佳。</p>

<h2 id="toc_1">改进</h2>

<p>　　设法降低训练集之间的相关性。<strong>随机森林</strong>是一种改进的Bagging。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Boosting]]></title>
    <link href="http://zxcoder.xyz/14592400407522.html"/>
    <updated>2016-03-29T16:27:20+08:00</updated>
    <id>http://zxcoder.xyz/14592400407522.html</id>
    <content type="html"><![CDATA[
<p>　　Boosting是一种Adaptive basis-function Model (ABM)。ABM模型可以用下面的式子概括：<br/>
\[f(x)=w_0+\sum_{m=1}^{M} w_m \phi_m(x) \qquad(1)\]<br/>
　　这里的\(\phi_m(x)\)是从数据中学习而来的第\(m\)个基函数。ABM通过多个模型的线性组合来得到<strong>非线性</strong>的模型。<br/>
　　Boosting是一种求ABM的贪心算法。Boosting中的基函数也叫做weak learner。Weak learner可以是任意的分类或回归模型，最常采用的是CART。</p>

<h2 id="toc_0">前向分步算法</h2>

<p>　　为了得到ABM模型，实际上就是要求\(M\)个基函数，以及它们对应的系数\(w_m\)。Boosting算法通过极小化损失函数来求这样一个模型。其优化目标可用下式表示：<br/>
\[\min_{f}=\sum_{i=1}^{N}L(y_i,f(x_i))\qquad (2)\]<br/>
式(2)中\(L\)是损失函数，\(N\)是训练样本数。具体的Boosting算法采用了不同的损失函数。前向分步的意思就是，通过\(M\)次迭代，向前逐步求式(1)中的\(f(x)\)，而不会回头来进行调整。在第\(m\)次迭代时得到的就是：<br/>
\[f_{m}(x)=f_{m-1}(x)+\alpha_m\phi_m(x) \qquad(3)\]<br/>
而最终求得的模型则是\(f(x)=f_M(x)\)。<br/>
　　在迭代的过程中逐步逼近优化目标，也就是在第\(m\)次迭代时，求\(\phi_m(x)\)及其对应系数\(\alpha_m\)以优化以下损失函数：<br/>
\[\min_{\phi,\alpha}\sum_{i=1}^{N}L(y_i,f_{m-1}(x)+\alpha\phi(x)) \qquad(4)\]</p>

<h2 id="toc_1">AdaBoost</h2>

<h4 id="toc_2">推理</h4>

<p>　　AdaBoost(Adaptive Boost)采用的损失函数是指数损失函数。在前向分步算法的第\(m\)步，优化的目标就是极小化式(5)：<br/>
\[L_m(\phi)=\sum_{i=1}^{N} exp[-y_i(f_{m-1}(x_i)+\alpha\phi(x_i))]\qquad(5)\]<br/>
这里只讨论二分类问题，令\(y_i\in\{-1,1\}\)。<br/>
　　下面要描述的是如何求\(\alpha\)和\(\phi(x)\)以对上式作优化。<br/>
　　首先令<br/>
\[w_{m,i}=exp[-y_if_{m-1}(x_i)]\qquad(6)\]<br/>
\(w_{m,i}\)表示第\(m\)次迭代时，第\(i\)个训练样本的权重。于是(5)可以表示为：<br/>
\[L_m(\phi)=\sum_{i=1}^{N}w_{m,i}exp(-\alpha y_i \phi(x_i))\qquad(7)\]<br/>
对二分类问题式(7)可以改写成：<br/>
\[L_m=e^{-\alpha}\sum_{y_i=\phi(x_i)}^{}w_{m,i}+e^{\alpha}\sum_{y_i\neq \phi(x_i)}^{}w_{m,i}\\\<br/>
=(e^{\alpha}-e^{-\alpha})\sum_{i=1}^{N}w_{m,i}I(y_i\neq \phi(x_i))+e^{-\alpha}\sum_{i=1}^{N}w_{m,i}<br/>
\qquad(8)\]<br/>
于是可得，使\(L_m\)极小化的\(\phi_m\)就如<strong>(9)</strong>所示：<br/>
\[\phi_m(x)=arg\min_{\phi}\sum_{i=1}^{N}w_{m,i}I(y_i \neq \phi(x_i))\qquad(9)\]<br/>
接着求\(\phi_m(x)\)的系数\(\alpha_m\)。在式(8)上对\(\alpha\)求导，令导数为0：<br/>
\[\frac{d L_m}{d \alpha}=e^{\alpha}\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x))-e^{-\alpha}\sum_{i=1}^{N}w_{m,i}I(y_i=\phi_m(x_i))=0\qquad(10)\]<br/>
化简可得：<br/>
\[\alpha=\frac{1}{2}ln{\frac{\sum_{i=1}^{N}w_{m,i}I(y_i=\phi_m(x_i))}{\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x_i))}}\qquad(11)\]<br/>
上式中，分子是所有被\(\phi_m(x)\)正确分类的样本的权值之和，分母是所有被\(\phi_m(x)\)误分类的样本的权值之和。如果令\(\phi_m(x)\)的加权分类误差为<br/>
\[e_m=\sum_{i=1}^{N}w_{m,i}I(y_i\neq\phi_m(x))\qquad(12)\]<br/>
则在求\(\alpha_m\)时式(11)可简化为：<br/>
\[\alpha_m=\frac{1}{2}ln{\frac{1-e_m}{e_m}}\qquad(13)\]<br/>
权值\(\alpha_m\)表示模型\(\phi_m(x)\)在最终的组合模型中的话语权大小。当\(e_m&lt;0.5\)时，\(alpha_m&gt;0\)，且\(\alpha_m\)随着\(e_m\)的减小而增大，即误差越小的模型越权威。<br/>
　　到此我们找到了极小化\(L_m\)的weak learner\(\phi_m(x)\)和它的权重系数\(\alpha_m\)。顺势可以使用式(3)求得在第\(m\)次迭代时的\(f_m(x)\)。<br/>
　　前面我们定义了式(6)，即令\(w_{m,i}=exp[-y_i f_{m-1}(x_i)]\)。每一次都使用上一次迭代得到的\(f_{m-1}(x)\)，因此训练样本的权值在随着迭代不断地更新。若已求得\(f_{m}(x)\)，则在第\({m+1}\)次迭代求\(w_{m+1,i}\)的方法如下：<br/>
\[w_{m+1,i}=exp[-y_i f_{m}(x_i)]\\<br/>
=exp[-y_i(f_{m-1}(x_i) +\alpha_m\phi_m(x_i)  )]\\<br/>
=w_{m,i} * e^{-y_i \alpha_m \phi_m(x_i)}\qquad(14)\]<br/>
上式的效果就是让那些在\(\phi_m(x)\)上被正确分类的样本对应的权重值变小，使错误分类的样本的权重值变大。这就使得后续的学习偏重于处理较难分类的样本。<br/>
　　为了让\(w_{m,i}\)成为一种概率分布，还要进一步正规化：<br/>
\[w_{m+1,i}=\frac{w_{m,i}exp[-y_i\alpha_m\phi_m(x_i)]}{Z_m}\qquad(15)\\<br/>
Z_m=\sum_{i=1}^{N}w_{m,i}exp[-y_i\alpha_m\phi_m(x_i)]\qquad(16)<br/>
\]<br/>
　　在第一次迭代时要用到\(f_0(x)\)，因此预先定义\(f_0(x)=0\)。于是有\(w_{1,i}=exp[-y_if_0(x_i)]=1\)，对\(i=1,2,...N\)。正规化后有\(w_{1,i}=\frac{1}{N}\)。<br/>
　　</p>

<h4 id="toc_3">算法 Adaboost</h4>

<p><strong>输入</strong>：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)，\(y_i\in\{-1,+1\}\)。以及弱学习模型\(\phi(x)\)；迭代次数\(M\)。<br/>
<strong>输出</strong>：强分类器\(\Phi(x)\)。</p>

<p>1.初始化训练样本的权值分布：<br/>
\[D_1=(w_{11},...,w_{1i},...w_{1N}),\quad w_{1i}=\frac{1}{N},\quad i=1,2,...N\]<br/>
2.对\(m=1,2,...M\)<br/>
　　a.使用训练数据集和它的权值分布\(D_m\)训练学习模型，得到弱学习模型\(\phi_m(x)\)，此模型满足式(9)。<br/>
　　b.根据式(12)计算\(\phi_m(x)\)在训练数据集上的带权误差<br/>
　　\[e_m=\sum_{i=1}^N w_{m,i}I(\phi_m(x_i)\neq y_i)\]<br/>
　　c.根据式(13)计算\(\phi_m(x)\)的权重系数<br/>
　　\[\alpha_m=\frac{1}{2}ln\frac{1-e_m}{e_m}\]<br/>
　　d.根据式(15)和(16)更新训练数据集的权值分布<br/>
\[D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...w_{m+1,N})\\\<br/>
w_{m+1,i}=\frac{w_{m,i} exp(-y_i \alpha_m \phi_m(x_i))}{Z_m}\\\<br/>
Z_m=\sum_{i=1}^{N} w_{m,i}exp(-y_i \alpha_m h_m(x_i))\]<br/>
3.组合得到最终分类器<br/>
\[\Phi(x)=sign(f_M(x))=sign(\sum_{m=1}^{M}\alpha_m \phi_m(x))\]</p>

<h4 id="toc_4">局限性</h4>

<p>　　AdaBoost容易偏向于难以分类的样本，因此对Outlier很敏感，容易过拟合。</p>

<h2 id="toc_5">其他Boosting算法</h2>

<h4 id="toc_6">L2Boosting</h4>

<p>　　在前向分步算法中，当采用平方误差损失函数时，就得到了L2Boosting模型。在L2Boosting模型中，weak learner的权重系数通常都设为1。因此在前向分步算法的第\(m\)次迭代时要优化的目标就是求：<br/>
\[\phi_m=arg\min_{\phi}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\phi(x_i))\\ <br/>
L(y,f_{m-1}(x)+\phi(x))=[y-f_{m-1}(x)-\phi(x)]^2\]<br/>
也就是要寻求一个\(\phi_m\)，使得\(\phi_m(x)\)尽可能地靠近(拟合)\(y-f_{m-1}(x)\)。令\(r=y-f_{m-1}(x)\)为当前模型(\(f_{m-1}(x)\))拟合数据的<strong>残差</strong>。在第\(m\)次迭代时，样本\(i\)对应的残差为：<br/>
\[r_{m,i}=y_i-f_{m-1}(x_i)\]<br/>
L2Boosting学习的迭代过程，就是通过拟合残差来学习新的weak learner，并把这些weak learner线性组合以得到强模型。</p>

<h2 id="toc_7">梯度提升Gradient Boosting</h2>

<p>　　Boosting算法可以采用各种不同的损失函数。这些算法可以归结为一种统一的形式，即梯度提升算法。梯度提升算法可以使用任意的合理的<strong>可微</strong>损失函数。对Boosting模型，我们在每一次迭代时都是求\(\phi_m(x)\)和\(\alpha_m\)，尽可能地使<br/>
　　\[f_{m+1}(x)=f_m(x)+\alpha_m\phi_m(x)=y\]<br/>
即尽可能地让<br/>
\[\alpha_m(x_i)\phi_m(x_i)=y_i-f_m(x_i)\]<br/>
上式右边正是残差。如果采用损失函数<br/>
\[L=\frac{1}{2}\sum_{i=1}^{N}(y_i-f_m(x_i))^2\]<br/>
则此残差就是损失函数的负梯度值<br/>
\[-\frac{\partial L}{\partial f_m(x_i)}=y-f_m(x_i)\]<br/>
　　对于一般化的损失函数，梯度提升算法采用损失函数负梯度值作为残差的近似值(<strong>伪残差</strong>)。在训练样本(输入向量)及其伪残差形成的数据集上训练新的weak learner。<br/>
　　像其他的Boosting算法一样，梯度提升算法也采用前向分步算法。在第\(m\)次迭代时，要求weak learner \(\phi_m(x)\)以及它对应的权重系数\(\alpha_m\)，以极小化损失函数为目标。梯度提升通过拟合伪残差来得到\(\phi_m(x)\)，然后求解一维的(以\(\alpha_m\)为自变量)优化问题得到\(\alpha_m\)。<br/>
　　</p>

<h4 id="toc_8">算法 Gradient Boosting</h4>

<p><strong>输入</strong>：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)。以及weak learner模型\(\phi(x)\)；迭代次数\(M\)。<br/>
<strong>输出</strong>：\(f_M(x)\)。</p>

<p>1.初始化模型\(f_0(x)\)(一个常数)：<br/>
\[f_0(x)=arg\min_{\alpha}\sum_{i=1}^{N}L(y_i,\alpha)\]<br/>
2.对\(m=1,2,...M\)：<br/>
　　a.求各训练样本的伪残差：<br/>
\[r_{m,i}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\]<br/>
　　b.在训练集\(\{(x_1,r_{m,1}),(x_2,r_{m,2}),...(x_N,r_{m,N})\}\)上训练，得到模型\(\phi_m(x)\)。<br/>
　　c.解一维优化问题，求\(\alpha_m\)<br/>
\[\alpha_m=arg\min_{\alpha}\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\alpha\phi_m(x_i))\]<br/>
　　d.更新模型<br/>
\[f_m(x)=f_{m-1}(x)+\alpha_m\phi_m(x)\]<br/>
3.输出 \(f_M(x)\)。</p>

<h4 id="toc_9">正则化</h4>

<p>　　为了提高模型的泛化能力，可以采用多种正则化策略。<br/>
　　1.限制迭代次数\(M\)。迭代次数决定了基本模型的数量。较大的\(M\)能减小训练误差，但容易过拟合。可以单独采用一个验证集，在迭代过程中评估泛化误差。<br/>
　　2.Shrinkage<br/>
给新学得的模型加一个较小的系数，以限制其对最终模型的影响：<br/>
\[f_m(x)=f_{m-1}(x)+ v\alpha_m\phi_m(x)\qquad 0&lt;v\leq 1\]<br/>
这个\(v\)也就是learning rate。较小的\(v\)会减慢算法的收敛速度，但能得到更好的模型。<br/>
　　2.随机梯度提升<br/>
在迭代过程中选用训练集的一个子集。通常抽取原数据集一半大小的子集。<br/>
　　3.当采用决策树作为基本模型时，可以限制叶节点内的样本数。若节点样本数小于一定值，则不再划分。</p>

<h2 id="toc_10">GBDT</h2>

<p>　　Gradient Boosting如果采用决策树作为基本模型，就得到了Gradient Boosting Decision Tree(GBDT)。也就是说在梯度提升算法的迭代过程中，每次要找的基本模型就是一棵树。树的叶节点们把输入空间划分成多个子区域，并对各子区域分别估计响应值(如对回归问题求落入该区域样本y值的平均，对分类问题采取多数投票)。设\(\phi_m(x)\)是一棵决策树，它把输入空间划分成\(J\)个区域，则\(\phi_m(x)\)可以表示为<br/>
\[\phi_m(x)=\sum_{j=1}^{J}c_{m,j}I(x\in R_{m,j})\]<br/>
这里\(R_{m,j}\)表示树\(\phi_m(x)\)的第\(j\)个也节点(输入空间子区域)，\(c_{m,j}\)是其对应的响应值。<br/>
　　这也就是说，GBDT对梯度提升算法做了改进，不再是给模型\(\phi_m(x)\)一个参数\(\alpha_m\)，而是给它一组参数与它所划分的子空间一一对应。<br/>
　　当设置\(J=2\)，就是采用决策桩(Decision Stump)作为基本模型。通常设置\(4\leq J\leq 8\)。</p>

<h4 id="toc_11">算法 GBDT</h4>

<p><strong>输入</strong>：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)。以及weak learner决策树模型\(\phi(x)\)；迭代次数\(M\)。<br/>
<strong>输出</strong>：\(f_M(x)\)。</p>

<p>1.初始化模型\(f_0(x)\)(一个常数)：<br/>
\[f_0(x)=arg\min_{\alpha}\sum_{i=1}^{N}L(y_i,\alpha)\]<br/>
2.对\(m=1,2,...M\)：<br/>
　　a.求各训练样本的伪残差：<br/>
\[r_{m,i}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}\]<br/>
　　b.在训练集\(\{(x_1,r_{m,1}),(x_2,r_{m,2}),...(x_N,r_{m,N})\}\)上训练，得到决策树模型\(\phi_m(x)\)，它有\(J\)个子区域。<br/>
　　c.对\(j=1,2,...J\)，计算<br/>
　　\[c_{m,j}=arg\min_{c}\sum_{x_i\in R_{m,j}}L(y_i, (f_{m-1}+c))\]<br/>
　　d.更新模型<br/>
\[f_m(x)=f_{m-1}(x)+\sum_{j=1}^{J}c_{m,j}I(x\in R_{m,j})\]<br/>
3.输出模型<br/>
\[f_M(x)=\sum_{m=1}^{M}\phi_m(x)\\\<br/>
=\sum_{m=1}^{M}\sum_{j=1}^{J}c_{m,j}I(x\in R_{m,j})\]</p>

<p>　　</p>

<p>　</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stacked Generalization]]></title>
    <link href="http://zxcoder.xyz/14591708106532.html"/>
    <updated>2016-03-28T21:13:30+08:00</updated>
    <id>http://zxcoder.xyz/14591708106532.html</id>
    <content type="html"><![CDATA[
<p>　　使用多层模型，前一层的输出作为特征，当作后一层的输入。以两层模型为例。第一层使用两种分类器，生成两种meta-feature；第二层使用一个分类器，以这些meta-features为特征生成分类结果。训练集和测试集都要经历这两个层次。在第一层，对于单个分类器，可以采用k-fold来生成这个分类器对应的meta-feature。整个过程可以用以下两图表示，这里采用了2-fold。<br/>
　　<img src="http://7xrz9i.com1.z0.glb.clouddn.com/ensemblestackedGeneralization-Train.png" alt=""/><br/>
　　<img src="http://7xrz9i.com1.z0.glb.clouddn.com/ensemblestackedGeneralization-Test.png" alt=""/><br/>
　　C1和C2是第一层采用的分类器。<br/>
　　对<strong>训练集</strong>的操作：<br/>
　　1.划分训练集P为两个fold，PA，PB。<br/>
　　2.对于C1，生成F1：<br/>
　　　　以PA为训练集训练它，然后将C1作用于PB，生成部分meta-feature F1B；<br/>
　　　　以PB为训练集训练它，然后将C1作用于PA，生成部分meta-feature F1A；<br/>
　　　　组合以上两部分meta-feature，生成F1，a column。<br/>
　　3.对C2，生成F2：过程类似2。<br/>
　　4.组合F1和F2，生成含有两种feature（Attribute）的新数据集F，y还是原来的训练集的y。<br/>
　　对<strong>测试集</strong>：<br/>
　　同样要生成两种meta-feature。<br/>
　　1.可以用整个训练集分别训练C1和C2，然后将C1和C2作用于测试集。就像上图2那样。<br/>
　　2.也可以把上图1中得到的两个(多个)C1都作用于测试集，投票或求评价得到TF1；用同样的方法得到TF2。<br/>
　　3.组合TF1，TF2，生成TF，含两种属性。 </p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Random Forest]]></title>
    <link href="http://zxcoder.xyz/14591708106411.html"/>
    <updated>2016-03-28T21:13:30+08:00</updated>
    <id>http://zxcoder.xyz/14591708106411.html</id>
    <content type="html"><![CDATA[
<p>　　随机森林，以决策树为基本模型的Bagging策略。通过两个层次的随机性来降低基本模型的相关性，从而提高组合模型的性能。<br/>
　　1.在训练每棵树时，从训练数据集中有放回地抽取样本；<br/>
　　2.每次分裂时随机选取一个特征子集(随机子空间).<br/>
<strong>算法：</strong><br/>
输入：训练数据集\(T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}\)，其中\(x_i\in\chi \subseteq R^{n}\)，样本含有\(n\)个属性，\(y_i\in\{-1,+1\}\)。以及基本模型\(h(x)\)(决策树)、基本模型的数量\(M\)、特征子集的大小\(sn\)。<br/>
输出：强模型\(H(x)\)。<br/>
1.对于\(m=1,2,...M\)<br/>
　　a.有放回地随机抽取训练数据集\(T_i\)；<br/>
　　b.用\(T_i\)训练\(h_i(x)\)，每次分裂时随机选取特征子集；<br/>
2.组合这M个模型，得强模型。</p>

<p><strong>Caution:</strong><br/>
1.对于分类问题，可以令特征子集的大小为\(sn=\sqrt{n}\)；对于回归问题可以令\(sn=\frac{n}{3}\)。<br/>
2.对分类问题，最终模型可以是M个模型的Major Vote；对于回归问题，可以对它们的结果求平均。<br/>
<strong>3.</strong>可以用随机森林来评估属性的重要程度。</p>

<p><strong>Feature Importance</strong><br/>
随机森林评估属性的重要性的算法。<br/>
输入：训练数据集\(T\)，在\(T\)上得到的随机森林模型\(H(x)\)。<br/>
输出：属性重要性。<br/>
1.计算\(T\)中样本的平均Out-of-bag error \(oob\).<br/>
2.对\(j=1,2,...F\)<br/>
　　a.对\(T\)中所有样本的属性\(j\)做Permute(Shuffle)，得到临时样本集合\(T&#39;\)，求\(T&#39;\)的Out-of-bag error \(oob_j\).<br/>
　　b.属性\(j\)的重要性得分是\(FS_j=oob-oob_j\)<br/>
3.输出所有属性的\(FS\)。值越大越重要。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Logistic 回归]]></title>
    <link href="http://zxcoder.xyz/14591708106259.html"/>
    <updated>2016-03-28T21:13:30+08:00</updated>
    <id>http://zxcoder.xyz/14591708106259.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[线性回归]]></title>
    <link href="http://zxcoder.xyz/14591708105958.html"/>
    <updated>2016-03-28T21:13:30+08:00</updated>
    <id>http://zxcoder.xyz/14591708105958.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
</feed>
